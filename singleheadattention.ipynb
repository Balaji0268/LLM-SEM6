{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dj_3ThONfP1v",
        "outputId": "280dd700-af47-424b-dd35-2d3b40b8aee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tokens:\n",
            " [[1. 0. 1. 0.]\n",
            " [0. 2. 0. 2.]\n",
            " [1. 1. 1. 1.]]\n",
            "\n",
            "Attention weights:\n",
            " [[0.07478316 0.23613828 0.68907856]\n",
            " [0.04191368 0.16370204 0.79438427]\n",
            " [0.0211171  0.13177896 0.84710393]]\n",
            "\n",
            "Output of single-head attention:\n",
            " [[1.52505401 3.19576466 2.57757996 1.3192114 ]\n",
            " [1.54470518 3.23312057 2.63780485 1.38672871]\n",
            " [1.55863388 3.26078451 2.67211186 1.41891149]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example token embeddings (3 tokens, each of dimension 4)\n",
        "X = np.array([\n",
        "    [1, 0, 1, 0],   # token 1\n",
        "    [0, 2, 0, 2],   # token 2\n",
        "    [1, 1, 1, 1]    # token 3\n",
        "], dtype=float)\n",
        "\n",
        "d_model = X.shape[1]\n",
        "\n",
        "# Weight matrices for Query, Key, Value\n",
        "W_Q = np.random.rand(d_model, d_model)\n",
        "W_K = np.random.rand(d_model, d_model)\n",
        "W_V = np.random.rand(d_model, d_model)\n",
        "\n",
        "# Step 1: Create Q, K, V\n",
        "Q = X @ W_Q\n",
        "K = X @ W_K\n",
        "V = X @ W_V\n",
        "\n",
        "# Step 2: Scaled dot-product attention\n",
        "scores = Q @ K.T / np.sqrt(d_model)\n",
        "\n",
        "# Softmax function\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "attention_weights = softmax(scores)\n",
        "\n",
        "# Step 3: Final output\n",
        "output = attention_weights @ V\n",
        "\n",
        "print(\"Input tokens:\\n\", X)\n",
        "print(\"\\nAttention weights:\\n\", attention_weights)\n",
        "print(\"\\nOutput of single-head attention:\\n\", output)\n"
      ]
    }
  ]
}