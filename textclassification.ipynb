{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_nnidq3MijQ",
        "outputId": "c621992c-543a-42d8-94a2-23a8c55eb1ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " \n",
            "Natural Language Processing is a field of Artificial Intelligence.\n",
            "It helps computers understand human language.\n",
            "Tokenization is the first step in NLP.\n",
            "Embeddings convert words into numbers.\n",
            "NLP is widely used in chatbots and translation systems.\n",
            "\n",
            "\n",
            "Summary:\n",
            "\n",
            "- NLP is widely used in chatbots and translation systems\n",
            "- Natural Language Processing is a field of Artificial Intelligence\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Input text\n",
        "text = \"\"\"\n",
        "Natural Language Processing is a field of Artificial Intelligence.\n",
        "It helps computers understand human language.\n",
        "Tokenization is the first step in NLP.\n",
        "Embeddings convert words into numbers.\n",
        "NLP is widely used in chatbots and translation systems.\n",
        "\"\"\"\n",
        "\n",
        "# 1. Sentence tokenization\n",
        "sentences = re.split(r'[.!?]', text)\n",
        "sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "# 2. Word tokenization + cleaning\n",
        "words = re.findall(r'\\w+', text.lower())\n",
        "\n",
        "# 3. Word frequency (features)\n",
        "word_freq = Counter(words)\n",
        "\n",
        "# 4. Score each sentence (classification-style scoring)\n",
        "sentence_scores = {}\n",
        "\n",
        "for sentence in sentences:\n",
        "    score = 0\n",
        "    for word in re.findall(r'\\w+', sentence.lower()):\n",
        "        score += word_freq[word]\n",
        "    sentence_scores[sentence] = score\n",
        "\n",
        "# 5. Select top sentences (summary)\n",
        "summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:2]\n",
        "\n",
        "# Output\n",
        "print(\"Original Text:\\n\", text)\n",
        "print(\"\\nSummary:\\n\")\n",
        "\n",
        "for s in summary_sentences:\n",
        "    print(\"-\", s)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple NER using Text Classification approach\n",
        "\n",
        "sentence = \"Balaji studies at IIT Madras in India\"\n",
        "\n",
        "# Tokenization\n",
        "words = sentence.split()\n",
        "\n",
        "# Entity dictionaries (training knowledge)\n",
        "persons = {\"Balaji\"}\n",
        "organizations = {\"IIT\"}\n",
        "locations = {\"India\", \"Madras\"}\n",
        "\n",
        "def classify_word(word):\n",
        "    if word in persons:\n",
        "        return \"PERSON\"\n",
        "    elif word in organizations:\n",
        "        return \"ORGANIZATION\"\n",
        "    elif word in locations:\n",
        "        return \"LOCATION\"\n",
        "    else:\n",
        "        return \"OTHER\"\n",
        "\n",
        "print(\"Sentence:\", sentence)\n",
        "print(\"\\nNamed Entities:\\n\")\n",
        "\n",
        "for word in words:\n",
        "    label = classify_word(word)\n",
        "    print(f\"{word:10} -> {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jX1H94IMkdL",
        "outputId": "8316822e-80c1-4b68-dc90-870a3b36b18c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Balaji studies at IIT Madras in India\n",
            "\n",
            "Named Entities:\n",
            "\n",
            "Balaji     -> PERSON\n",
            "studies    -> OTHER\n",
            "at         -> OTHER\n",
            "IIT        -> ORGANIZATION\n",
            "Madras     -> LOCATION\n",
            "in         -> OTHER\n",
            "India      -> LOCATION\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Knowledge base (paragraph)\n",
        "text = \"\"\"\n",
        "NLP stands for Natural Language Processing.\n",
        "It helps computers understand human language.\n",
        "Tokenization is the first step in NLP.\n",
        "Embeddings convert words into numbers.\n",
        "\"\"\"\n",
        "\n",
        "# Question\n",
        "question = \"What is tokenization?\"\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]\n",
        "\n",
        "# Function to convert text to word features\n",
        "def text_to_features(text):\n",
        "    words = re.findall(r'\\w+', text.lower())\n",
        "    return Counter(words)\n",
        "\n",
        "# Features for question\n",
        "question_features = text_to_features(question)\n",
        "\n",
        "# Score sentences (classification-like)\n",
        "scores = {}\n",
        "\n",
        "for sentence in sentences:\n",
        "    sentence_features = text_to_features(sentence)\n",
        "    score = 0\n",
        "    for word in question_features:\n",
        "        score += min(question_features[word], sentence_features.get(word, 0))\n",
        "    scores[sentence] = score\n",
        "\n",
        "# Select best answer\n",
        "best_answer = max(scores, key=scores.get)\n",
        "\n",
        "print(\"Question:\", question)\n",
        "print(\"\\nAnswer:\")\n",
        "print(best_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i4G5rtnMkZ2",
        "outputId": "ac410fe8-3190-4107-eedc-947d9408676d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is tokenization?\n",
            "\n",
            "Answer:\n",
            "Tokenization is the first step in NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ogPTTzrIMkXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ABZOcy_eMkVG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}